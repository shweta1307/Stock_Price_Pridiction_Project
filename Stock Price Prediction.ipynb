{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c025a943",
   "metadata": {},
   "source": [
    "# DATA SCIENCE INTERN @BHARAT INTERN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59812b",
   "metadata": {},
   "source": [
    "### AUTHOR : Shweta Santosh Kapile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b402f2",
   "metadata": {},
   "source": [
    "# TASK 1 : STOCK PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f14f64",
   "metadata": {},
   "source": [
    "## STEP 1 : IMPORTING LIBRARIES AND DATA TO BE USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fe173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries to be used\n",
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # data preprocessing\n",
    "import matplotlib.pyplot as plt # data visualization library\n",
    "import seaborn as sns # data visualization library\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # for normalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0310c3d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Google Stocks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Downloads\\Stock Price Prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/Stock%20Price%20Prediction.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mGoogle Stocks.csv\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# data_importing\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Downloads/Stock%20Price%20Prediction.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m) \u001b[39m# fetching first 10 rows of dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Google Stocks.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Google Stocks.csv') # data_importing\n",
    "df.head(10) # fetching first 10 rows of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f948f7",
   "metadata": {},
   "source": [
    "## STEP 2 : GATHERING INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of data\n",
    "print(\"Shape of data:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec59878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical description of data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db53322",
   "metadata": {},
   "source": [
    "### There are no null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45349ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date','open','close']] # Extracting required columns\n",
    "df['date'] = pd.to_datetime(df['date'].apply(lambda x: x.split()[0])) # converting object dtype of date column to datetime dtype\n",
    "df.set_index('date',drop=True,inplace=True) # Setting date column as index\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting open and closing price on date index\n",
    "fig, ax =plt.subplots(1,2,figsize=(20,7))\n",
    "ax[0].plot(df['open'],label='Open',color='green')\n",
    "ax[0].set_xlabel('Date',size=15)\n",
    "ax[0].set_ylabel('Price',size=15)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(df['close'],label='Close',color='red')\n",
    "ax[1].set_xlabel('Date',size=15)\n",
    "ax[1].set_ylabel('Price',size=15)\n",
    "ax[1].legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77919e0",
   "metadata": {},
   "source": [
    "## STEP 3 : DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing all the values of all columns using MinMaxScaler\n",
    "MMS = MinMaxScaler()\n",
    "df[df.columns] = MMS.fit_transform(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and test set\n",
    "training_size = round(len(df) * 0.75) # Selecting 75 % for training and 25 % for testing\n",
    "training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c507274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[:training_size]\n",
    "test_data  = df[training_size:]\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d05650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequence of data for training and testing\n",
    "\n",
    "def create_sequence(dataset):\n",
    "  sequences = []\n",
    "  labels = []\n",
    "\n",
    "  start_idx = 0\n",
    "\n",
    "  for stop_idx in range(50,len(dataset)): # Selecting 50 rows at a time\n",
    "    sequences.append(dataset.iloc[start_idx:stop_idx])\n",
    "    labels.append(dataset.iloc[stop_idx])\n",
    "    start_idx += 1\n",
    "  return (np.array(sequences),np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, train_label = create_sequence(train_data) \n",
    "test_seq, test_label = create_sequence(test_data)\n",
    "train_seq.shape, train_label.shape, test_seq.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f37ed4",
   "metadata": {},
   "source": [
    "## STEP 4 :  CREATING LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported Sequential from keras.models \n",
    "model = Sequential()\n",
    "# importing Dense, Dropout, LSTM, Bidirectional from keras.layers \n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape = (train_seq.shape[1], train_seq.shape[2])))\n",
    "\n",
    "model.add(Dropout(0.1)) \n",
    "model.add(LSTM(units=50))\n",
    "\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308278a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fitting the model by iterating the dataset over 100 times(100 epochs)\n",
    "model.fit(train_seq, train_label, epochs=100,validation_data=(test_seq, test_label), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac85b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the values after running the model\n",
    "test_predicted = model.predict(test_seq)\n",
    "test_predicted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversing normalization/scaling on predicted data \n",
    "test_inverse_predicted = MMS.inverse_transform(test_predicted)\n",
    "test_inverse_predicted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688e1e2",
   "metadata": {},
   "source": [
    "## STEP 5 :  VISUALIZING ACTUAL VS PREDICTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging actual and predicted data for better visualization\n",
    "df_merge = pd.concat([df.iloc[-264:].copy(),\n",
    "                          pd.DataFrame(test_inverse_predicted,columns=['open_predicted','close_predicted'],\n",
    "                                       index=df.iloc[-264:].index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversing normalization/scaling \n",
    "df_merge[['open','close']] = MMS.inverse_transform(df_merge[['open','close']])\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e552918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the actual open and predicted open prices on date index\n",
    "df_merge[['open','open_predicted']].plot(figsize=(10,6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.ylabel('Stock Price',size=15)\n",
    "plt.title('Actual vs Predicted for open price',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19365b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting the actual close and predicted close prices on date index \n",
    "df_merge[['close','close_predicted']].plot(figsize=(10,6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.ylabel('Stock Price',size=15)\n",
    "plt.title('Actual vs Predicted for close price',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b3580",
   "metadata": {},
   "source": [
    "## STEP 6. PREDICTING UPCOMING 10 DAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe and adding 10 days to existing index \n",
    "\n",
    "df_merge = df_merge.append(pd.DataFrame(columns=df_merge.columns,\n",
    "                                        index=pd.date_range(start=df_merge.index[-1], periods=11, freq='D', closed='right')))\n",
    "df_merge['2021-06-09':'2021-06-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6822e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a DataFrame and filling values of open and close column\n",
    "upcoming_prediction = pd.DataFrame(columns=['open','close'],index=df_merge.index)\n",
    "upcoming_prediction.index=pd.to_datetime(upcoming_prediction.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_seq = test_seq[-1:]\n",
    "\n",
    "for i in range(-10,0):\n",
    "  up_pred = model.predict(curr_seq)\n",
    "  upcoming_prediction.iloc[i] = up_pred\n",
    "  curr_seq = np.append(curr_seq[0][1:],up_pred,axis=0)\n",
    "  curr_seq = curr_seq.reshape(test_seq[-1:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37525dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversing Normalization/scaling\n",
    "upcoming_prediction[['open','close']] = MMS.inverse_transform(upcoming_prediction[['open','close']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7187b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Upcoming Open price on date index\n",
    "fig,ax=plt.subplots(figsize=(10,5))\n",
    "ax.plot(df_merge.loc['2021-04-01':,'open'],label='Current Open Price')\n",
    "ax.plot(upcoming_prediction.loc['2021-04-01':,'open'],label='Upcoming Open Price')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "ax.set_xlabel('Date',size=15)\n",
    "ax.set_ylabel('Stock Price',size=15)\n",
    "ax.set_title('Upcoming Open price prediction',size=15)\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Upcoming Close price on date index\n",
    "fig,ax=plt.subplots(figsize=(10,5))\n",
    "ax.plot(df_merge.loc['2021-04-01':,'close'],label='Current close Price')\n",
    "ax.plot(upcoming_prediction.loc['2021-04-01':,'close'],label='Upcoming close Price')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "ax.set_xlabel('Date',size=15)\n",
    "ax.set_ylabel('Stock Price',size=15)\n",
    "ax.set_title('Upcoming close price prediction',size=15)\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a008259",
   "metadata": {},
   "source": [
    "# THANK YOU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
